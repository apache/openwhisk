# Licensed to the Apache Software Foundation (ASF) under one or more contributor
# license agreements; and to You under the Apache License, Version 2.0.

# default application configuration file for akka
include "logging"

akka.http {
    client {
        parsing.illegal-header-warnings = off
        parsing.max-chunk-size = 50m
        parsing.max-content-length = 50m
    }

    host-connection-pool {
        max-connections = 128
        max-open-requests = 1024
    }
}

#kamon related configuration
kamon {

    metric {
        tick-interval = 1 second
    }

    statsd {
        # Interval between metrics data flushes to StatsD. It's value must be equal or greater than the
        # kamon.metrics.tick-interval setting.
        flush-interval = 1 second

        # Max packet size for UDP metrics data sent to StatsD.
        max-packet-size = 1024 bytes

        # Subscription patterns used to select which metrics will be pushed to StatsD. Note that first, metrics
        # collection for your desired entities must be activated under the kamon.metrics.filters settings.
        includes {
            actor      =  [ "*" ]
            trace      =  [ "*" ]
            dispatcher =  [ "*" ]
        }

        simple-metric-key-generator {
            # Application prefix for all metrics pushed to StatsD. The default namespacing scheme for metrics follows
            # this pattern:
            #    application.host.entity.entity-name.metric-name
            application = "openwhisk-statsd"
        }
    }

    modules {
        kamon-statsd.auto-start = yes
    }
}

whisk {
    # kafka related configuration, the complete list of parameters is here:
    # https://kafka.apache.org/documentation/#brokerconfigs
    kafka {
        replication-factor = 1

        // Used to control the cadence of the consumer lag check interval
        consumer-lag-check-interval = 60 seconds

        // The following settings are passed "raw" to the respective Kafka client. Dashes are replaced by dots.
        common {
            security-protocol = PLAINTEXT
            ssl-endpoint-identification-algorithm = "" // restores pre-kafka 2.0.0 default
        }
        producer {
            acks = 1
            request-timeout-ms = 30000
            metadata-max-age-ms = 15000
            # max-request-size is defined programatically for producers related to the "completed" and "invoker" topics
            # as ${whisk.activation.kafka.payload.max} + ${whisk.activation.kafka.serdes-overhead}. All other topics use
            # the default of 1 MB.
        }
        consumer {
            session-timeout-ms = 30000
            heartbeat-interval-ms = 10000
            enable-auto-commit = false
            auto-offset-reset = earliest
            request-timeout-ms = 30000

            max-poll-interval-ms = 1800000 // 30 minutes

            // The maximum amount of time the server will block before answering
            // the fetch request if there isn't sufficient data to immediately
            // satisfy the requirement given by fetch.min.bytes.
            // (default is 500, default of fetch.min.bytes is 1)
            // On changing fetch.min.bytes, a high value for fetch.max.wait.ms,
            // could increase latency of activations.
            // A low value will cause excessive busy-waiting.
            fetch-max-wait-ms = 500
        }

        topics {
            cache-invalidation {
                segment-bytes   =  536870912
                retention-bytes = 1073741824
                retention-ms    = 300000
            }
            completed {
                segment-bytes   =  536870912
                retention-bytes = 1073741824
                retention-ms    = 3600000
                # max-message-bytes is defined programatically as ${whisk.activation.kafka.payload.max} +
                # ${whisk.activation.kafka.serdes-overhead}.
            }
            health {
                segment-bytes   =  536870912
                retention-bytes = 1073741824
                retention-ms    = 3600000
            }
            invoker {
                segment-bytes     =  536870912
                retention-bytes   = 1073741824
                retention-ms      =  172800000
                # max-message-bytes is defined programatically as ${whisk.activation.kafka.payload.max} +
                # ${whisk.activation.kafka.serdes-overhead}.
            }
            events {
                segment-bytes   =  536870912
                retention-bytes = 1073741824
                retention-ms    = 3600000
            }
        }
    }
    # db related configuration
    db {
        actions-ddoc = "whisks.v2.1.0"
        activations-ddoc = "whisks.v2.1.0"
        activations-filter-ddoc = "whisks-filters.v2.1.0"

        # Size limit for inlined attachments. Attachments having size less than this would
        # be inlined with there content encoded in attachmentName
        max-inline-size = 16 k
    }

    # CouchDB related configuration
    # For example:
    # couchdb {
    #     protocol = http          # One of "https" or "http"
    #     host     = servername    # DB Host
    #     port     = 5984          # DB Port
    #     username =
    #     password =
    #     provider =               # Either "Cloudant" or "CouchDB"
    #     databases {              # Database names used for various entity classes
    #        WhiskAuth       =
    #        WhiskEntity     =
    #        WhiskActivation =
    #     }
    #}

    # CosmosDB related configuration
    # For example:
    # cosmosdb {
    #     endpoint   =               # Endpoint URL like https://<account>.documents.azure.com:443/
    #     key        =               # Access key
    #     db         =               # Database name
    #     throughput = 1000          # Throughput configure for each collection within this db
    #}

    # transaction ID related configuration
    transactions {
        header = "X-Request-ID"
    }
    # action runtimes configuration
    runtimes {
        bypass-pull-for-local-images = false
        local-image-prefix = "whisk"
    }

    user-events {
        enabled = false
    }

    activation {
        payload {
            max = 1048576
        }
        # Action responses sent through Kafka can contain up to 3018 bytes of metadata
        #   CompletionMessage
        #       base                            71
        #       TransactionId
        #           id                          32
        #           start                       13
        #           extraLogging                5
        #       WhiskActivation
        #           base                        368
        #           activationId                32
        #           subject                     256
        #           namespace                   256
        #           entity name                 256
        #           path                        3x256+2=770
        #           initTime                    64
        #           waitTime                    64
        #           duration                    64
        #           kind                        64
        #           limits                      64*3=192
        #           version                     64x3+2=194
        #       InvokerInstanceId
        #           instance                    64
        #           uniqueName + displayName    253 (max pod name length in Kube)
        serdes-overhead = 6036 // 3018 bytes of metadata * 2 for extra headroom
    }

    # action timelimit configuration
    time-limit {
        min = 100 ms
        max = 5 m
        std = 1 m
    }

    # action memory configuration
    memory {
        min = 128 m
        max = 512 m
        std = 256 m
    }

    # action log-limit configuration
    log-limit {
        min = 0 m
        max = 10 m
        std = 10 m
    }

    # action concurrency-limit configuration
    concurrency-limit {
        min = 1
        max = 1
        std = 1
    }

    mesos {
        master-url = "http://localhost:5050" //your mesos master
        master-public-url = "http://localhost:5050" // if mesos-link-log-message == true, this link will be included with the static log message (may or may not be different from master-url)
        role = "*" //see http://mesos.apache.org/documentation/latest/roles/#associating-frameworks-with-roles
        mesos-link-log-message = true //If true, display a link to mesos in the static log message, otherwise do not include a link to mesos.
        constraints = [] //placement constraint strings to use for managed containers e.g. ["att1 LIKE v1", "att2 UNLIKE v2"]
        blackbox-constraints = [] //placement constraints to use for blackbox containers
        constraint-delimiter = " "//used to parse constraint strings
        teardown-on-exit = true //set to true to disable the mesos framework on system exit; set for false for HA deployments
        offer-refuse-duration = 5 seconds //minimum time until an offer will arrive again at a particular invoker
        timeouts {
            failover = 0 seconds  //Timeout allowed for framework to reconnect after disconnection.
            task-launch = 45 seconds //timeout for creating mesos tasks (containers)
            task-delete = 30 seconds //timeout for destroying mesos tasks (containers)
            subscribe = 10 seconds //timeout for framework subscription handshake
            teardown = 30 seconds //timeout for framework teardown
        }
        health-check {#Remove health-section section to disable healthchecks at action containers
            port-index = 0 //should always be port 0 (action container should only listen on 1 port)
            delay = 0 seconds //the amount of time (in seconds) to wait until starting checking the task.
            interval = 1 seconds //the interval (in seconds) between check attempts.
            timeout = 1 seconds //the amount of time (in seconds) to wait for the check to complete
            grace-period = 25 seconds //the amount of time after the task is launched during which health check failures are ignored.
            max-consecutive-failures = 3 //the number of consecutive failures until the task is killed by the executor.
        }
    }

    logstore {
        #SplunkLogStore configuration
        #splunk {
        #    host = "splunkhost"                   #splunk api hostname
        #    port = 8089                           #splunk api port
        #    username = "splunkapiusername"        #splunk api username
        #    password = "splunkapipassword"        #splunk api password
        #    index = "splunkindex"                 #splunk index name
        #    log-timestamp-field = "log_timestamp" #splunk field where timestamp is stored (to reflect log event generated time, not splunk's _time)
        #    log-stream-field = "log_stream"       #splunk field where stream is stored (stdout/stderr)
        #    log-message-field = "log_message"     #splunk field where log message is stored
        #    activation-id-field = "activation_id" #splunk field where activation id is stored
        #    query-constraints = ""                #additional constraints for splunk queries
        #    query-timestamp-offset-seconds = ""   #splunk query will be broadened by this 2*<offset value>; e.g. "earliest_time=activation.start - offset" and "latest_time=activation.end + offset"
        #    disableSNI = false                    #if true, disables hostname validation and cert validation (in case splunk api endpoint is using a self signed cert)
        #}
    }

    # tracing configuration
    tracing {
        cache-expiry = 30 seconds #how long to keep spans in cache. Set to appropriate value to trace long running requests
        #Zipkin configuration. Uncomment following to enable zipkin based tracing
        #zipkin {
        #   url = "http://localhost:9411" //url to connecto to zipkin server
             //sample-rate to decide a request is sampled or not.
             //sample-rate 0.5 eqauls to sampling 50% of the requests
             //sample-rate of 1 means 100% sampling.
             //sample-rate of 0 means no sampling
        #   sample-rate = "0.01" // sample 1% of requests by default
        #}
    }
}
#placeholder for test overrides so that tests can override defaults in application.conf (todo: move all defaults to reference.conf)
test {
}
